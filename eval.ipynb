{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from MixedAutoencoder import Mixer, MixedAutoencoder\n",
    "import MixedAutoencoder\n",
    "from DataCleaning import *\n",
    "import DataCleaning\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pywebio.output import *\n",
    "from pywebio.input import *\n",
    "from pywebio import start_server\n",
    "import io\n",
    "matplotlib.use('agg')\n",
    "seed = 42\n",
    "MixedAutoencoder.setRandom(seed)\n",
    "DataCleaning.setRandom(seed)\n",
    "base_path = \".\"\n",
    "mixer = Mixer(base_path)\n",
    "\n",
    "num_sets = 4\n",
    "latent_dim = 8\n",
    "model_shape = []\n",
    "encoder = \"demo\"\n",
    "label = f'demo_{num_sets}_{latent_dim}_[{\"_\".join([str(s) for s in model_shape])}]'\n",
    "\n",
    "model, extra = mixer.load_from_label(label)\n",
    "model_3d, _extra = mixer.load_from_label(label+\"_3d\")\n",
    "#print(len(extra[\"columns\"][encoder]))\n",
    "key_list = extra[\"key_list\"]\n",
    "columns = extra[\"columns\"]\n",
    "\n",
    "sets = [k for k in key_list if k != encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(f'{base_path}/data/16PF/data.csv', sep=\"\\t\")\n",
    "data = clean_data(data)\n",
    "data = data.sample(frac=1)\n",
    "split_data = split(data, columns)\n",
    "train, test = make_train_test(split_data, 0.8)\n",
    "full_columns = {\"main\": list(data), \"main2\": list(data)}\n",
    "full_data = split(data, full_columns)\n",
    "full_train, full_test = make_train_test(full_data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function MixedAutoencoder.get_accuracy at 0x000002848756ADD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function MixedAutoencoder.get_accuracy at 0x000002848756ADD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Binary accuracy: 0.40778210759162903\n"
     ]
    }
   ],
   "source": [
    "model.show_subtotal_accuracy([\"set1\", \"set2\", \"set3\", \"set4\"], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary accuracy: 0.7905327476318705\n"
     ]
    }
   ],
   "source": [
    "model.show_subtotal_binary_accuracy([\"set1\", \"set2\", \"set3\", \"set4\"], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('set1', 'set1')\n",
      "221/221 [==============================] - 0s 530us/step - loss: 0.6940\n",
      "compared to average: 1.5601943501759679\n",
      "('set1', 'set2')\n",
      "221/221 [==============================] - 0s 517us/step - loss: 0.9193\n",
      "compared to average: 1.5142659627953745\n",
      "('set1', 'set3')\n",
      "221/221 [==============================] - 0s 530us/step - loss: 0.9656\n",
      "compared to average: 1.4522765837104072\n",
      "('set1', 'set4')\n",
      "221/221 [==============================] - 0s 557us/step - loss: 0.9663\n",
      "compared to average: 1.4666161044837516\n",
      "('set1', 'demo')\n",
      "221/221 [==============================] - 0s 521us/step - loss: 0.8550\n",
      "compared to average: 1.5450980392156863\n",
      "('set2', 'set1')\n",
      "221/221 [==============================] - 0s 552us/step - loss: 0.9266\n",
      "compared to average: 1.5601943501759679\n",
      "('set2', 'set2')\n",
      "221/221 [==============================] - 0s 537us/step - loss: 0.6467\n",
      "compared to average: 1.5142659627953745\n",
      "('set2', 'set3')\n",
      "221/221 [==============================] - 0s 521us/step - loss: 0.9088\n",
      "compared to average: 1.4522765837104072\n",
      "('set2', 'set4')\n",
      "221/221 [==============================] - 0s 525us/step - loss: 0.9331\n",
      "compared to average: 1.4666161044837516\n",
      "('set2', 'demo')\n",
      "221/221 [==============================] - 0s 525us/step - loss: 0.8102\n",
      "compared to average: 1.5450980392156863\n",
      "('set3', 'set1')\n",
      "221/221 [==============================] - 0s 512us/step - loss: 0.9453\n",
      "compared to average: 1.5601943501759679\n",
      "('set3', 'set2')\n",
      "221/221 [==============================] - 0s 512us/step - loss: 0.8683\n",
      "compared to average: 1.5142659627953745\n",
      "('set3', 'set3')\n",
      "221/221 [==============================] - 0s 521us/step - loss: 0.7026\n",
      "compared to average: 1.4522765837104072\n",
      "('set3', 'set4')\n",
      "221/221 [==============================] - 0s 524us/step - loss: 0.9131\n",
      "compared to average: 1.4666161044837516\n",
      "('set3', 'demo')\n",
      "221/221 [==============================] - 0s 512us/step - loss: 0.8149\n",
      "compared to average: 1.5450980392156863\n",
      "('set4', 'set1')\n",
      "221/221 [==============================] - 0s 512us/step - loss: 0.9583\n",
      "compared to average: 1.5601943501759679\n",
      "('set4', 'set2')\n",
      "221/221 [==============================] - 0s 575us/step - loss: 0.8954\n",
      "compared to average: 1.5142659627953745\n",
      "('set4', 'set3')\n",
      "221/221 [==============================] - 0s 514us/step - loss: 0.9189\n",
      "compared to average: 1.4522765837104072\n",
      "('set4', 'set4')\n",
      "221/221 [==============================] - 0s 521us/step - loss: 0.6720\n",
      "compared to average: 1.4666161044837516\n",
      "('set4', 'demo')\n",
      "221/221 [==============================] - 0s 521us/step - loss: 0.8261\n",
      "compared to average: 1.5450980392156863\n",
      "('demo', 'set1')\n",
      "221/221 [==============================] - 0s 516us/step - loss: 0.9995\n",
      "compared to average: 1.5601943501759679\n",
      "('demo', 'set2')\n",
      "221/221 [==============================] - 0s 519us/step - loss: 0.9469\n",
      "compared to average: 1.5142659627953745\n",
      "('demo', 'set3')\n",
      "221/221 [==============================] - 0s 521us/step - loss: 0.9845\n",
      "compared to average: 1.4522765837104072\n",
      "('demo', 'set4')\n",
      "221/221 [==============================] - 0s 516us/step - loss: 0.9879\n",
      "compared to average: 1.4666161044837516\n",
      "('demo', 'demo')\n",
      "221/221 [==============================] - 0s 553us/step - loss: 0.4793\n",
      "compared to average: 1.5450980392156863\n"
     ]
    }
   ],
   "source": [
    "model.eval_set(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 1.1674 - val_loss: 1.3654 - accuracy: 0.2778\n",
      "epoch 1\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 1.0664 - val_loss: 1.2752 - accuracy: 0.3153\n",
      "epoch 2\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 1.0072 - val_loss: 1.2163 - accuracy: 0.3215\n",
      "epoch 3\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.9562 - val_loss: 1.1674 - accuracy: 0.3314\n",
      "epoch 4\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.9107 - val_loss: 1.1278 - accuracy: 0.3444\n",
      "epoch 5\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.8737 - val_loss: 1.0961 - accuracy: 0.3552\n",
      "epoch 6\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.8457 - val_loss: 1.0722 - accuracy: 0.3678\n",
      "epoch 7\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.8243 - val_loss: 1.0531 - accuracy: 0.3742\n",
      "epoch 8\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.8061 - val_loss: 1.0389 - accuracy: 0.3764\n",
      "epoch 9\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7901 - val_loss: 1.0275 - accuracy: 0.3761\n",
      "epoch 10\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7771 - val_loss: 1.0180 - accuracy: 0.3802\n",
      "epoch 11\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7666 - val_loss: 1.0089 - accuracy: 0.3889\n",
      "epoch 12\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7581 - val_loss: 0.9996 - accuracy: 0.3909\n",
      "epoch 13\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7514 - val_loss: 0.9908 - accuracy: 0.3914\n",
      "epoch 14\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7458 - val_loss: 0.9819 - accuracy: 0.3930\n",
      "epoch 15\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7408 - val_loss: 0.9738 - accuracy: 0.3933\n",
      "epoch 16\n",
      "442/442 [==============================] - 2s 2ms/step - loss: 0.7366 - val_loss: 0.9670 - accuracy: 0.3972\n",
      "epoch 17\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7331 - val_loss: 0.9617 - accuracy: 0.3996\n",
      "epoch 18\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7298 - val_loss: 0.9574 - accuracy: 0.4017\n",
      "epoch 19\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7270 - val_loss: 0.9538 - accuracy: 0.4044\n",
      "epoch 20\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7244 - val_loss: 0.9508 - accuracy: 0.4059\n",
      "epoch 21\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7223 - val_loss: 0.9482 - accuracy: 0.4071\n",
      "epoch 22\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7204 - val_loss: 0.9461 - accuracy: 0.4075\n",
      "epoch 23\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7189 - val_loss: 0.9442 - accuracy: 0.4091\n",
      "epoch 24\n",
      "442/442 [==============================] - 1s 2ms/step - loss: 0.7175 - val_loss: 0.9424 - accuracy: 0.4092\n"
     ]
    }
   ],
   "source": [
    "autoencoder = mixer.make_new({\"main\": [], \"main2\": []}, latent_dim, {\"main\": len(full_columns[\"main\"]), \"main2\": len(full_columns[\"main2\"])})\n",
    "settings = {\n",
    "    \"training\": [[\"$all\", \"$all\", True, True]]#[[[k1], [k2 for k2 in keys if k1 != k2], True, True] for k1 in keys],\n",
    "    #\"encoder_proximity_training\": [[\"$all\", True]],\n",
    "    #\"plot\": [True, 3, [0, 1, 2]]\n",
    "}\n",
    "autoencoder.train_set(full_train, 25, autoencoder.make_train_config(settings = settings), batch_size = 64, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy: 0.44711536169052124\n",
      "Binary accuracy: 0.8344270757328582\n"
     ]
    }
   ],
   "source": [
    "autoencoder.show_total_accuracy(full_test)\n",
    "autoencoder.show_total_binary_accuracy(full_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encoderVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
