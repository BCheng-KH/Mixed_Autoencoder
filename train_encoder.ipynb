{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MixedAutoencoder import Mixer, MixedAutoencoder\n",
    "import MixedAutoencoder\n",
    "from DataCleaning import *\n",
    "import DataCleaning\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "seed = 42\n",
    "MixedAutoencoder.setRandom(seed)\n",
    "DataCleaning.setRandom(seed)\n",
    "base_path = \".\"\n",
    "mixer = Mixer(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_sets = 4\n",
    "latent_dim = 7\n",
    "model_shape = []\n",
    "base_path = \".\"\n",
    "label = f'demo_{num_sets}_{latent_dim}_[{\"_\".join([str(s) for s in model_shape])}]'\n",
    "demo_size = 20\n",
    "demo_shape = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "base_key_list = [f'set{str(i)}' for i in range(1, num_sets+1)]\n",
    "demo_key_list = ['demo']\n",
    "\n",
    "data = pd.read_csv(f'{base_path}/data/16PF/data.csv', sep=\"\\t\")\n",
    "data = clean_data(data)\n",
    "data = data.sample(frac=1)\n",
    "base_column_keys = split_strat(data,num_sets, base_key_list)\n",
    "demo_column_keys = {demo_key_list[0]: split_n_strat(data, demo_size)}\n",
    "split_data = split(data, base_column_keys)\n",
    "train, test = make_train_test(split_data, 0.8)\n",
    "input_dims = get_input_dims(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary accuracy: 0.49382968122050225\n",
      "epoch 0\n",
      "442/442 [==============================] - 4s 8ms/step - loss: 1.1438 - val_loss: 1.2548 - accuracy: 0.2699\n",
      "epoch 1\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.9835 - val_loss: 1.1096 - accuracy: 0.3304\n",
      "epoch 2\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.9127 - val_loss: 1.0332 - accuracy: 0.3555\n",
      "epoch 3\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.8614 - val_loss: 0.9894 - accuracy: 0.3701\n",
      "epoch 4\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.8286 - val_loss: 0.9672 - accuracy: 0.3808\n",
      "epoch 5\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.8102 - val_loss: 0.9515 - accuracy: 0.3874\n",
      "epoch 6\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.7990 - val_loss: 0.9388 - accuracy: 0.3928\n",
      "epoch 7\n",
      "442/442 [==============================] - 4s 5ms/step - loss: 0.7912 - val_loss: 0.9288 - accuracy: 0.3973\n",
      "epoch 8\n",
      "442/442 [==============================] - 4s 5ms/step - loss: 0.7852 - val_loss: 0.9219 - accuracy: 0.3963\n",
      "epoch 9\n",
      "442/442 [==============================] - 4s 5ms/step - loss: 0.7805 - val_loss: 0.9174 - accuracy: 0.3969\n",
      "epoch 10\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.7767 - val_loss: 0.9143 - accuracy: 0.3985\n",
      "epoch 11\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.7734 - val_loss: 0.9120 - accuracy: 0.4009\n",
      "epoch 12\n",
      "442/442 [==============================] - 4s 5ms/step - loss: 0.7706 - val_loss: 0.9106 - accuracy: 0.4030\n",
      "epoch 13\n",
      "442/442 [==============================] - 4s 5ms/step - loss: 0.7684 - val_loss: 0.9099 - accuracy: 0.4042\n",
      "epoch 14\n",
      "442/442 [==============================] - 4s 5ms/step - loss: 0.7665 - val_loss: 0.9096 - accuracy: 0.4063\n",
      "epoch 15\n",
      "442/442 [==============================] - 4s 5ms/step - loss: 0.7652 - val_loss: 0.9095 - accuracy: 0.4064\n",
      "epoch 16\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.7640 - val_loss: 0.9093 - accuracy: 0.4061\n",
      "epoch 17\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.7630 - val_loss: 0.9092 - accuracy: 0.4062\n",
      "epoch 18\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.7622 - val_loss: 0.9090 - accuracy: 0.4053\n",
      "epoch 19\n",
      "442/442 [==============================] - 3s 5ms/step - loss: 0.7615 - val_loss: 0.9088 - accuracy: 0.4043\n",
      "Binary accuracy: 0.7918400716558569\n"
     ]
    }
   ],
   "source": [
    "model_shapes = {k: model_shape for k in base_key_list}\n",
    "autoencoder_set = mixer.make_new(model_shapes, latent_dim, input_dims)\n",
    "autoencoder_set.show_total_binary_accuracy(test)\n",
    "settings = {\n",
    "    \"training\": [[\"$all\", \"$all\", True, True]]#[[[k1], [k2 for k2 in keys if k1 != k2], True, True] for k1 in keys],\n",
    "    #\"encoder_proximity_training\": [[\"$all\", True]],\n",
    "    #\"plot\": [True, 3, [0, 1, 2]]\n",
    "}\n",
    "autoencoder_set.train_set(train, 20, autoencoder_set.make_train_config(settings = settings), batch_size = 64, verbose=True)\n",
    "autoencoder_set.show_total_binary_accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "442/442 [==============================] - 2s 5ms/step - loss: 1.4708 - val_loss: 1.2545 - accuracy: 0.3084\n",
      "epoch 1\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 1.2022 - val_loss: 1.0653 - accuracy: 0.3505\n",
      "epoch 2\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 1.0431 - val_loss: 0.9349 - accuracy: 0.3914\n",
      "epoch 3\n",
      "442/442 [==============================] - 3s 4ms/step - loss: 0.9587 - val_loss: 0.8645 - accuracy: 0.4067\n",
      "epoch 4\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.9206 - val_loss: 0.8291 - accuracy: 0.4166\n",
      "epoch 5\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.9032 - val_loss: 0.8129 - accuracy: 0.4277\n",
      "epoch 6\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8944 - val_loss: 0.8052 - accuracy: 0.4328\n",
      "epoch 7\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8900 - val_loss: 0.8013 - accuracy: 0.4364\n",
      "epoch 8\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8877 - val_loss: 0.7991 - accuracy: 0.4353\n",
      "epoch 9\n",
      "442/442 [==============================] - 3s 4ms/step - loss: 0.8865 - val_loss: 0.7977 - accuracy: 0.4362\n",
      "epoch 10\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8859 - val_loss: 0.7968 - accuracy: 0.4389\n",
      "epoch 11\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8856 - val_loss: 0.7963 - accuracy: 0.4395\n",
      "epoch 12\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8854 - val_loss: 0.7959 - accuracy: 0.4413\n",
      "epoch 13\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8853 - val_loss: 0.7957 - accuracy: 0.4423\n",
      "epoch 14\n",
      "442/442 [==============================] - 2s 3ms/step - loss: 0.8852 - val_loss: 0.7955 - accuracy: 0.4423\n",
      "epoch 15\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8852 - val_loss: 0.7954 - accuracy: 0.4427\n",
      "epoch 16\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8852 - val_loss: 0.7953 - accuracy: 0.4427\n",
      "epoch 17\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8852 - val_loss: 0.7952 - accuracy: 0.4435\n",
      "epoch 18\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8852 - val_loss: 0.7952 - accuracy: 0.4435\n",
      "epoch 19\n",
      "442/442 [==============================] - 2s 4ms/step - loss: 0.8852 - val_loss: 0.7952 - accuracy: 0.4435\n",
      "Encoder binary accuracy: 0.7881934203992327\n",
      "Decoder binary accuracy: 0.8194406385630564\n"
     ]
    }
   ],
   "source": [
    "demo_split_data = split(data, demo_column_keys)\n",
    "dtrain, dtest = make_train_test(demo_split_data, 0.8)\n",
    "demo_input_dims = get_input_dims(dtrain)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "demo_shapes = {k: demo_shape for k in demo_key_list}\n",
    "autoencoder_set = mixer.add_new(autoencoder_set, demo_shapes, demo_input_dims)\n",
    "\n",
    "settings = {\n",
    "    \"training\": [[demo_key_list, \"$all\", True, False], [\"$all\", demo_key_list, False, True]]#[[[k1], [k2 for k2 in keys if k1 != k2], True, True] for k1 in keys],\n",
    "    #\"encoder_proximity_training\": [[\"$all\", True]],\n",
    "    #\"plot\": [True, 3, [0, 1, 2]]\n",
    "}\n",
    "autoencoder_set.train_set(train | dtrain, 20, autoencoder_set.make_train_config(settings = settings), batch_size = 64, verbose=True)\n",
    "autoencoder_set.show_binary_accuracy(demo_key_list, test | dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/encoder_set1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/decoder_set1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/encoder_set2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/decoder_set2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/encoder_set3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/decoder_set3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/encoder_set4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/decoder_set4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/encoder_demo\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./Models/model_demo_4_7_[]/decoder_demo\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mixer.save_to_label(autoencoder_set, extra = {\"columns\" : base_column_keys|demo_column_keys}, label = label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encoderVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
